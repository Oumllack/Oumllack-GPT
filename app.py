import os
import sys
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import streamlit as st

# Configuration de la page (DOIT √äTRE EN PREMIER)
st.set_page_config(
    page_title="Oumllack-GPT",
    page_icon="ü§ñ",
    layout="wide"
)

# Afficher le r√©pertoire de travail
st.write(f"R√©pertoire de travail: {os.getcwd()}")

# Style CSS personnalis√©
st.markdown("""
    <style>
    /* Style g√©n√©ral */
    .stApp {
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
        color: white;
    }
    
    /* Style de la sidebar */
    .css-1d391kg {
        background: rgba(255, 255, 255, 0.05);
    }
    
    /* Style des messages */
    .chat-message {
        padding: 1.5rem;
        border-radius: 1rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .chat-message.user {
        background: linear-gradient(135deg, #4a90e2 0%, #357abd 100%);
        color: white;
        margin-left: auto;
        max-width: 80%;
    }
    
    .chat-message.bot {
        background: rgba(255, 255, 255, 0.1);
        color: white;
        margin-right: auto;
        max-width: 80%;
    }
    
    /* Style des inputs et boutons */
    .stTextInput>div>div>input {
        background-color: rgba(255, 255, 255, 0.1);
        color: white;
        border: 1px solid rgba(255, 255, 255, 0.2);
        border-radius: 0.5rem;
    }
    
    .stButton>button {
        background: linear-gradient(135deg, #4a90e2 0%, #357abd 100%);
        color: white;
        border: none;
        padding: 0.5rem 1rem;
        border-radius: 0.5rem;
        transition: all 0.3s ease;
    }
    </style>
""", unsafe_allow_html=True)

# Titre de l'application
st.title("ü§ñ Oumllack-GPT")
st.markdown("Un chatbot IA entra√Æn√© sur des dialogues de films")

# Sidebar pour les param√®tres
with st.sidebar:
    st.header("Param√®tres")
    temperature = st.slider("Temp√©rature", 0.1, 1.0, 0.7, 0.1)
    max_length = st.slider("Longueur maximale", 50, 200, 100, 10)
    
    st.markdown("---")
    st.markdown("### √Ä propos")
    st.markdown("""
    Oumllack-GPT est un chatbot IA entra√Æn√© sur des dialogues de films.
    Il utilise le mod√®le GPT-2 fine-tun√© pour g√©n√©rer des r√©ponses naturelles.
    """)

@st.cache_resource
def load_model():
    try:
        # Utiliser le chemin absolu
        model_path = "/Users/spacegreen/Desktop/NLP project/chatbot_model"
        st.write(f"Tentative de chargement depuis: {model_path}")
        
        if not os.path.exists(model_path):
            st.error(f"Dossier non trouv√©: {model_path}")
            return None, None
            
        # V√©rifier les fichiers
        required_files = ["model.safetensors", "config.json", "tokenizer_config.json", "vocab.json", "merges.txt"]
        for file in required_files:
            file_path = os.path.join(model_path, file)
            if os.path.exists(file_path):
                st.write(f"‚úÖ Fichier trouv√©: {file}")
            else:
                st.error(f"‚ùå Fichier manquant: {file}")
                return None, None
        
        st.write("Chargement du tokenizer...")
        tokenizer = GPT2Tokenizer.from_pretrained(model_path, local_files_only=True)
        st.write("Tokenizer charg√©")
        
        st.write("Chargement du mod√®le...")
        model = GPT2LMHeadModel.from_pretrained(model_path, local_files_only=True)
        st.write("Mod√®le charg√©")
        
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = 'left'
        model.eval()
        
        st.success("Mod√®le et tokenizer charg√©s avec succ√®s!")
        return model, tokenizer
    except Exception as e:
        st.error(f"Erreur: {str(e)}")
        import traceback
        st.error(f"Traceback: {traceback.format_exc()}")
        return None, None

def generate_response(prompt):
    try:
        model, tokenizer = load_model()
        if model is None or tokenizer is None:
            return "Erreur: Mod√®le non charg√©"
            
        st.write("G√©n√©ration de la r√©ponse...")
        inputs = tokenizer.encode(prompt, return_tensors="pt")
        outputs = model.generate(
            inputs,
            max_length=100,
            temperature=0.7,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(prompt, "").strip()
    except Exception as e:
        st.error(f"Erreur lors de la g√©n√©ration: {str(e)}")
        import traceback
        st.error(f"Traceback: {traceback.format_exc()}")
        return f"Erreur: {str(e)}"

# Initialisation des √©tats de session
if "messages" not in st.session_state:
    st.session_state.messages = []

# Zone de chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Zone de saisie
if prompt := st.chat_input("√âcrivez votre message ici"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
        
    with st.chat_message("assistant"):
        with st.spinner("G√©n√©ration de la r√©ponse..."):
            response = generate_response(prompt)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response}) 